{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports/Installs**"
      ],
      "metadata": {
        "id": "yYxz4-bYq-nb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_BxamkKyQSd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d9176f5-08b3-4c2e-8a1e-02b00f6108b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "竢ｬ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "沒ｦ Installing...\n",
            "沒 Adjusting configuration...\n",
            "洸ｹ Patching environment...\n",
            "竢ｲ Done in 0:00:15\n",
            "沐 Restarting kernel...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch\n",
            "  Downloading torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.10.0-py3-none-any.whl (9.9 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx\n",
            "  Downloading networkx-3.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.6.3)\n",
            "Collecting cmake\n",
            "  Downloading cmake-3.26.0-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lit\n",
            "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting mpmath>=0.19\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93598 sha256=10b833645c380575dcf941920e7a07fc3c673158cb7b979597b9a1d5a51bc9b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/ee/80/1520ca86c3557f70e5504b802072f7fc3b0e2147f376b133ed\n",
            "Successfully built lit\n",
            "Installing collected packages: mpmath, lit, cmake, typing-extensions, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, MarkupSafe, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, jinja2, triton, torch\n",
            "Successfully installed MarkupSafe-2.1.2 cmake-3.26.0 filelock-3.10.0 jinja2-3.1.2 lit-16.0.0 mpmath-1.3.0 networkx-3.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.11.1 torch-2.0.0 triton-2.0.0 typing-extensions-4.5.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.15.1-cp39-cp39-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.24.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.9/site-packages (from torchvision) (2.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from torchvision) (2.28.2)\n",
            "Collecting pillow!=8.3.*,>=5.3.0\n",
            "  Downloading Pillow-9.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (2.14.3)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.4.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (1.11.1)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (8.5.0.96)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (3.10.0)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (11.7.101)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch==2.0.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchvision) (65.6.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchvision) (0.38.4)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.0->torchvision) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.0->torchvision) (3.26.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->torch==2.0.0->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/site-packages (from sympy->torch==2.0.0->torchvision) (1.3.0)\n",
            "Installing collected packages: pillow, numpy, torchvision\n",
            "Successfully installed numpy-1.24.2 pillow-9.4.0 torchvision-0.15.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting d2l==1.0.0b0\n",
            "  Downloading d2l-1.0.0b0-py3-none-any.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m141.6/141.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from d2l==1.0.0b0) (1.24.2)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from d2l==1.0.0b0) (2.28.2)\n",
            "Collecting jupyter\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpytorch\n",
            "  Downloading gpytorch-1.9.1-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle>=1.2.0\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting linear-operator>=0.2.0\n",
            "  Downloading linear_operator-0.3.0-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m155.6/155.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting notebook\n",
            "  Downloading notebook-6.5.3-py3-none-any.whl (529 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipykernel\n",
            "  Downloading ipykernel-6.22.0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m150.0/150.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipywidgets\n",
            "  Downloading ipywidgets-8.0.4-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m137.8/137.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nbconvert\n",
            "  Downloading nbconvert-7.2.10-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m275.2/275.2 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-console\n",
            "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.4.1-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m120.9/120.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.39.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/site-packages (from matplotlib->d2l==1.0.0b0) (9.4.0)\n",
            "Collecting packaging>=20.0\n",
            "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m299.7/299.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing>=2.3.1\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting importlib-resources>=3.2.0\n",
            "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
            "Collecting python-dateutil>=2.7\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting traitlets\n",
            "  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2020.1\n",
            "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->d2l==1.0.0b0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->d2l==1.0.0b0) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->d2l==1.0.0b0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->d2l==1.0.0b0) (1.26.15)\n",
            "Collecting zipp>=3.1.0\n",
            "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.9/site-packages (from linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (2.0.0)\n",
            "Collecting six>=1.5\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting jupyter-client>=6.1.12\n",
            "  Downloading jupyter_client-8.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m102.9/102.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipython>=7.23.1\n",
            "  Downloading ipython-8.11.0-py3-none-any.whl (793 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m793.3/793.3 kB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting debugpy>=1.6.5\n",
            "  Downloading debugpy-1.6.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nest-asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting comm>=0.1.1\n",
            "  Downloading comm-0.1.2-py3-none-any.whl (6.5 kB)\n",
            "Collecting pyzmq>=20\n",
            "  Downloading pyzmq-25.0.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tornado>=6.1\n",
            "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m424.0/424.0 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m280.2/280.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-core!=5.0.*,>=4.12\n",
            "  Downloading jupyter_core-5.3.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting widgetsnbextension~=4.0\n",
            "  Downloading widgetsnbextension-4.0.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyterlab-widgets~=3.0\n",
            "  Downloading jupyterlab_widgets-3.0.5-py3-none-any.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m384.3/384.3 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prompt-toolkit>=3.0.30\n",
            "  Downloading prompt_toolkit-3.0.38-py3-none-any.whl (385 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tinycss2\n",
            "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
            "Collecting nbformat>=5.1\n",
            "  Downloading nbformat-5.8.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nbclient>=0.5.0\n",
            "  Downloading nbclient-0.7.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyterlab-pygments\n",
            "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
            "Collecting defusedxml\n",
            "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.9/site-packages (from nbconvert->jupyter->d2l==1.0.0b0) (2.1.2)\n",
            "Collecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.12.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m132.2/132.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mistune<3,>=2.0.3\n",
            "  Downloading mistune-2.0.5-py2.py3-none-any.whl (24 kB)\n",
            "Collecting importlib-metadata>=3.6\n",
            "  Downloading importlib_metadata-6.1.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.9/site-packages (from nbconvert->jupyter->d2l==1.0.0b0) (3.1.2)\n",
            "Collecting pandocfilters>=1.4.1\n",
            "  Downloading pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting bleach\n",
            "  Downloading bleach-6.0.0-py3-none-any.whl (162 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prometheus-client\n",
            "  Downloading prometheus_client-0.16.0-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m122.5/122.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting terminado>=0.8.3\n",
            "  Downloading terminado-0.17.1-py3-none-any.whl (17 kB)\n",
            "Collecting Send2Trash>=1.8.0\n",
            "  Downloading Send2Trash-1.8.0-py3-none-any.whl (18 kB)\n",
            "Collecting ipython-genutils\n",
            "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting argon2-cffi\n",
            "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n",
            "Collecting nbclassic>=0.4.7\n",
            "  Downloading nbclassic-0.5.3-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m316.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting joblib>=1.1.1\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pexpect>4.3\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stack-data\n",
            "  Downloading stack_data-0.6.2-py3-none-any.whl (24 kB)\n",
            "Collecting backcall\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pickleshare\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting decorator\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting platformdirs>=2.5\n",
            "  Downloading platformdirs-3.1.1-py3-none-any.whl (14 kB)\n",
            "Collecting jupyter-server>=1.8\n",
            "  Downloading jupyter_server-2.5.0-py3-none-any.whl (366 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m366.8/366.8 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting notebook-shim>=0.1.0\n",
            "  Downloading notebook_shim-0.2.2-py3-none-any.whl (13 kB)\n",
            "Collecting fastjsonschema\n",
            "  Downloading fastjsonschema-2.16.3-py3-none-any.whl (23 kB)\n",
            "Collecting jsonschema>=2.6\n",
            "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wcwidth\n",
            "  Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB)\n",
            "Collecting ptyprocess\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (2.14.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (3.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (11.7.99)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (1.11.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (11.7.91)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/site-packages (from torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (3.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (0.38.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (65.6.3)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (3.26.0)\n",
            "Collecting argon2-cffi-bindings\n",
            "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.4-py3-none-any.whl (37 kB)\n",
            "Collecting webencodings\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting parso<0.9.0,>=0.8.0\n",
            "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
            "  Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting attrs>=17.4.0\n",
            "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websocket-client\n",
            "  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-events>=0.4.0\n",
            "  Downloading jupyter_events-0.6.3-py3-none-any.whl (18 kB)\n",
            "Collecting jupyter-server-terminals\n",
            "  Downloading jupyter_server_terminals-0.4.4-py3-none-any.whl (13 kB)\n",
            "Collecting anyio>=3.1.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l==1.0.0b0) (1.15.1)\n",
            "Collecting executing>=1.2.0\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.1.0\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting pure-eval\n",
            "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/site-packages (from sympy->torch>=1.11->linear-operator>=0.2.0->gpytorch->d2l==1.0.0b0) (1.3.0)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->d2l==1.0.0b0) (2.21)\n",
            "Collecting rfc3986-validator>=0.1.1\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting rfc3339-validator\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting python-json-logger>=2.0.4\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Collecting pyyaml>=5.3\n",
            "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m661.8/661.8 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isoduration\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting webcolors>=1.11\n",
            "  Downloading webcolors-1.12-py3-none-any.whl (9.9 kB)\n",
            "Collecting fqdn\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting jsonpointer>1.13\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting uri-template\n",
            "  Downloading uri_template-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting arrow>=0.15.0\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616821 sha256=5403793cdad6e9abdfb01f68b7220f1090d2978fd14dbf2e42c3041cccc7ee9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/50/6c/0a82c1358b4da2dbd9c1bb17e0f89467db32812ab236dbf6d5\n",
            "Successfully built gym\n",
            "Installing collected packages: webencodings, wcwidth, Send2Trash, pytz, pure-eval, ptyprocess, pickleshare, mistune, ipython-genutils, fastjsonschema, executing, backcall, zipp, widgetsnbextension, websocket-client, webcolors, uri-template, traitlets, tornado, tinycss2, threadpoolctl, soupsieve, sniffio, six, scipy, rfc3986-validator, pyzmq, pyyaml, python-json-logger, pyrsistent, pyparsing, pygments, psutil, prompt-toolkit, prometheus-client, platformdirs, pexpect, parso, pandocfilters, packaging, nest-asyncio, kiwisolver, jupyterlab-widgets, jupyterlab-pygments, jsonpointer, joblib, fqdn, fonttools, defusedxml, decorator, debugpy, cycler, contourpy, cloudpickle, attrs, terminado, scikit-learn, rfc3339-validator, qtpy, python-dateutil, matplotlib-inline, jupyter-core, jsonschema, jedi, importlib-resources, importlib-metadata, gym, comm, bleach, beautifulsoup4, asttokens, argon2-cffi-bindings, anyio, stack-data, pandas, nbformat, matplotlib, jupyter-server-terminals, jupyter-client, arrow, argon2-cffi, nbclient, isoduration, ipython, nbconvert, ipykernel, qtconsole, jupyter-events, jupyter-console, ipywidgets, jupyter-server, notebook-shim, nbclassic, notebook, jupyter, linear-operator, gpytorch, d2l\n",
            "Successfully installed Send2Trash-1.8.0 anyio-3.6.2 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 arrow-1.2.3 asttokens-2.2.1 attrs-22.2.0 backcall-0.2.0 beautifulsoup4-4.12.0 bleach-6.0.0 cloudpickle-2.2.1 comm-0.1.2 contourpy-1.0.7 cycler-0.11.0 d2l-1.0.0b0 debugpy-1.6.6 decorator-5.1.1 defusedxml-0.7.1 executing-1.2.0 fastjsonschema-2.16.3 fonttools-4.39.2 fqdn-1.5.1 gpytorch-1.9.1 gym-0.21.0 importlib-metadata-6.1.0 importlib-resources-5.12.0 ipykernel-6.22.0 ipython-8.11.0 ipython-genutils-0.2.0 ipywidgets-8.0.4 isoduration-20.11.0 jedi-0.18.2 joblib-1.2.0 jsonpointer-2.3 jsonschema-4.17.3 jupyter-1.0.0 jupyter-client-8.1.0 jupyter-console-6.6.3 jupyter-core-5.3.0 jupyter-events-0.6.3 jupyter-server-2.5.0 jupyter-server-terminals-0.4.4 jupyterlab-pygments-0.2.2 jupyterlab-widgets-3.0.5 kiwisolver-1.4.4 linear-operator-0.3.0 matplotlib-3.7.1 matplotlib-inline-0.1.6 mistune-2.0.5 nbclassic-0.5.3 nbclient-0.7.2 nbconvert-7.2.10 nbformat-5.8.0 nest-asyncio-1.5.6 notebook-6.5.3 notebook-shim-0.2.2 packaging-23.0 pandas-1.5.3 pandocfilters-1.5.0 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 platformdirs-3.1.1 prometheus-client-0.16.0 prompt-toolkit-3.0.38 psutil-5.9.4 ptyprocess-0.7.0 pure-eval-0.2.2 pygments-2.14.0 pyparsing-3.0.9 pyrsistent-0.19.3 python-dateutil-2.8.2 python-json-logger-2.0.7 pytz-2022.7.1 pyyaml-6.0 pyzmq-25.0.2 qtconsole-5.4.1 qtpy-2.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 scikit-learn-1.2.2 scipy-1.10.1 six-1.16.0 sniffio-1.3.0 soupsieve-2.4 stack-data-0.6.2 terminado-0.17.1 threadpoolctl-3.1.0 tinycss2-1.2.1 tornado-6.2 traitlets-5.9.0 uri-template-1.2.0 wcwidth-0.2.6 webcolors-1.12 webencodings-0.5.1 websocket-client-1.5.1 widgetsnbextension-4.0.5 zipp-3.15.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cycler",
                  "dateutil",
                  "debugpy",
                  "defusedxml",
                  "importlib_resources",
                  "ipython_genutils",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "pexpect",
                  "pickleshare",
                  "psutil",
                  "tornado",
                  "wcwidth",
                  "zipp"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install d2l==1.0.0b0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbV9155tKF7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830166b4-b06e-4b5d-ba28-efd860724eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamoseley018\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install wandb -qU\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl9Rf7gIC-Lb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa4c3ce-0296-477d-ead7-89e2cad8fd16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UySZvCsHhycT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from torchsummary import summary\n",
        "from d2l import torch as d2l\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import gc\n",
        "import skimage\n",
        "import h5py\n",
        "import math\n",
        "import PIL\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss Functions**"
      ],
      "metadata": {
        "id": "IGcZ_Vm5n5Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedDiceLoss(nn.Module):\n",
        "    def __init__(self, weight0=1, weight1=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight0 = weight0\n",
        "        self.weight1 = weight1\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        loss0 = 0\n",
        "        loss1 = 0\n",
        "\n",
        "        imgSize = input.size()[1]\n",
        "\n",
        "        correctPixels = 0\n",
        "\n",
        "        for i, segment in enumerate(input):\n",
        "            falseNeg0 = 0\n",
        "            falsePos0 = 0\n",
        "            truePos0 = 0\n",
        "\n",
        "            falseNeg1 = 0\n",
        "            falsePos1 = 0\n",
        "            truePos1 = 0\n",
        "\n",
        "            segment = torch.round(segment)\n",
        "\n",
        "            for j in range(imgSize):\n",
        "                for k in range(imgSize):\n",
        "                    if segment[j][k] == 0 and target[i][j][k] == 0:\n",
        "                        truePos0 += 1\n",
        "                    elif segment[i][j] == 0 and target[i][j][k] == 1:\n",
        "                        falsePos0 += 1\n",
        "                    elif segment[i][j] == 1 and target[i][j][k] == 0:\n",
        "                        falseNeg0 += 1\n",
        "\n",
        "                    if segment[j][k] == 1 and target[i][j][k] == 1:\n",
        "                        truePos1 += 1\n",
        "                    elif segment[i][j] == 1 and target[i][j][k] == 0:\n",
        "                        falsePos1 += 1\n",
        "                    elif segment[i][j] == 0 and target[i][j][k] == 1:\n",
        "                        falseNeg1 += 1\n",
        "\n",
        "            if truePos0 > 0:\n",
        "                loss0 += (2 * truePos0) / ((2 * truePos0) + falsePos0 + falseNeg0)\n",
        "\n",
        "            if truePos1 > 0:\n",
        "                loss1 += (2 * truePos1) / ((2 * truePos1) + falsePos1 + falseNeg1)\n",
        "\n",
        "            correctPixels += truePos0 + truePos1\n",
        "\n",
        "        loss0 /= input.size()[0]\n",
        "        loss1 /= input.size()[0]\n",
        "\n",
        "        return torch.as_tensor(1 - ((self.weight0 * loss0) + (self.weight1 * loss1))), correctPixels / (imgSize * imgSize * input.size()[0])"
      ],
      "metadata": {
        "id": "6p7aAli6D06P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBBsw05Qw7DO"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, weight0=1, weight1=1, gamma=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight0 = weight0\n",
        "        self.weight1 = weight1\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        loss = 0\n",
        "\n",
        "        predictions = torch.round(input)\n",
        "        accuratePreds = 0\n",
        "\n",
        "        #Takes negative average loss over each element of input\n",
        "        #Loss = ln(prediction) * (absolute loss ^ gamma) * class weight\n",
        "        #prediction is the predicted likelihood that the correct label is true\n",
        "        for i, el in enumerate(input):\n",
        "            #print(f\"{predictions[i]} {el}\")\n",
        "\n",
        "            if predictions[i] == target[i]:\n",
        "                accuratePreds += 1\n",
        "\n",
        "            if target[i] == 1:\n",
        "                loss += torch.log(el) * (abs(1 - el) ** self.gamma) * self.weight1\n",
        "            else:\n",
        "                loss += torch.log(1 - el) * (abs(0 - el) ** self.gamma) * self.weight0\n",
        "\n",
        "        return -1 * loss / len(input), accuratePreds / input.size()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsZV06GU0ue3"
      },
      "outputs": [],
      "source": [
        "class BalancedCELoss(nn.Module):\n",
        "    def __init__(self, weight0=1, weight1=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight0 = weight0\n",
        "        self.weight1 = weight1\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        loss = 0\n",
        "\n",
        "        predictions = torch.round(input)\n",
        "        accuratePreds = 0\n",
        "\n",
        "        #Takes negative average loss over each element of input\n",
        "        #Loss = ln(prediction) * class weight\n",
        "        #prediction is the predicted likelihood that the correct label is true\n",
        "        for i, el in enumerate(input):\n",
        "            if predictions[i] == target[i]:\n",
        "                accuratePreds += 1\n",
        "\n",
        "            if target[i] == 1:\n",
        "                loss += torch.log(el) * self.weight1\n",
        "            else:\n",
        "                loss += torch.log(1 - el) * self.weight0\n",
        "\n",
        "        return -1 * loss / len(input), accuratePreds / input.size()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Handling**"
      ],
      "metadata": {
        "id": "6g1MsLCCoDhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iO7WioSr-Yk"
      },
      "outputs": [],
      "source": [
        "class LITSBinaryDataset(Dataset):\n",
        "    def __init__(self, fileName):\n",
        "        super().__init__()\n",
        "\n",
        "        #Keeps a file pointer open throughout use\n",
        "        self.file = h5py.File(fileName, 'r')\n",
        "\n",
        "        #Precalculates length to reduce training computations\n",
        "        self.length = len(list(self.file.keys()))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.file[\"Slice\" + str(idx)][\"Slice\"]\n",
        "        segmentation = self.file[\"Slice\" + str(idx)][\"Segmentation\"]\n",
        "        label = self.file[\"Slice\" + str(idx)].attrs.get(\"ImageLabel\")\n",
        "\n",
        "        result = []\n",
        "\n",
        "        #Returns list containing slice data and image label\n",
        "        #Does not currently return segmentation data, will need to implement for decoder\n",
        "        result.append(torch.Tensor(data[...]).unsqueeze(0))\n",
        "        result.append(torch.Tensor(segmentation[...]))\n",
        "        result.append(torch.Tensor(label).squeeze(0))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def closeFile(self):\n",
        "        #Closes file once dataset is no longer being used\n",
        "        #Do not use class instance after this function is called\n",
        "        self.file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Network**"
      ],
      "metadata": {
        "id": "6adquwMWoH0B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-kqO-PVhycU"
      },
      "outputs": [],
      "source": [
        "class convBlock(nn.Module):\n",
        "    def __init__(self, inChannels, outChannels, batchNorm, strides, layerMean, layerDev, dropout) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        #Uses 2 convolutional layers for each block\n",
        "        self.conv1 = nn.Conv2d(inChannels, outChannels, kernel_size=3, padding=1, stride=strides)\n",
        "        self.conv2 = nn.Conv2d(outChannels, outChannels, kernel_size=3, padding=1)\n",
        "\n",
        "        #Initializes convolutional layers using hyperparameters for mean and standard deviation\n",
        "        nn.init.normal_(self.conv1.weight, mean=layerMean, std=layerDev)\n",
        "        nn.init.normal_(self.conv2.weight, mean=layerMean, std=layerDev)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout).to(device)\n",
        "\n",
        "        if(batchNorm):\n",
        "            self.bn1 = nn.BatchNorm2d(outChannels)\n",
        "        else:\n",
        "            self.bn1 = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = self.conv1(X)\n",
        "        if self.bn1:\n",
        "            Y = self.bn1(Y)\n",
        "        Y = F.relu(Y)\n",
        "\n",
        "        Y = self.conv2(Y)\n",
        "        if self.bn1:\n",
        "            Y = self.bn1(Y)\n",
        "        Y = F.relu(Y)\n",
        "\n",
        "        Y = self.dropout(Y)\n",
        "\n",
        "        return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0sgeg3EhycU"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, inChannels, outChannels, strides, dropout) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        #Creates a convBlock with batch norm and max pooling layer, directly from UNet paper\n",
        "        self.conv = convBlock(inChannels, outChannels, True, strides, 0, 0.025, dropout)\n",
        "        self.pool = nn.MaxPool2d(2, stride=None)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = self.conv(X)\n",
        "\n",
        "        #Only for use in testing, does not return skip connection data\n",
        "        #return self.pool(Y)\n",
        "\n",
        "        #Returns the average pool of Y for the next encoder block, Y for a skip connections\n",
        "        return self.pool(Y), Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rhGtpfGhycV"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, inChannels, outChannels, strides, dropout) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.convTrans = nn.ConvTranspose2d(inChannels, outChannels, 2, stride=2, padding=0)\n",
        "        self.conv = convBlock(outChannels, outChannels, True, strides, 0, 0.25, dropout)\n",
        "\n",
        "    def forward(self, X, skipConn):\n",
        "        Y = self.convTrans(X)\n",
        "        Y = torch.cat((Y, skipConn), dim=0)\n",
        "\n",
        "        return self.conv(Y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderNetwork(nn.Module):\n",
        "        def __init__(self, channels, strides, dropout, device) -> None:\n",
        "            super().__init__()\n",
        "\n",
        "            self.device = device\n",
        "            self.blocks = []\n",
        "\n",
        "            for i in range(len(channels) - 1):\n",
        "                self.blocks.append(DecoderBlock(channels[i], channels[i + 1], 1, dropout).to(device))\n",
        "\n",
        "        def forward(self, X, skipConnections):\n",
        "            if len(skipConnections) != len(self.blocks):\n",
        "                return None\n",
        "\n",
        "            y = X\n",
        "            for i in range(len(self.blocks)):\n",
        "                y = self.blocks[i](y, skipConnections[-(i + 1)])\n",
        "\n",
        "            return y"
      ],
      "metadata": {
        "id": "zFdMolDN9_Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBhZXZ3x8mnJ"
      },
      "outputs": [],
      "source": [
        "class EncoderNetwork(nn.Module):\n",
        "    def __init__(self, channels, strides, dropout, device) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        #Creates a list of encoder blocks w/ in and out channels specified by parameter\n",
        "        self.blocks = []\n",
        "        for i in range(len(channels)):\n",
        "            if i == 0:\n",
        "                self.blocks.append(EncoderBlock(1, channels[i], 1, dropout).to(device))\n",
        "            else:\n",
        "                self.blocks.append(EncoderBlock(channels[i - 1], channels[i], 1, dropout).to(device))\n",
        "\n",
        "        #Creates classification branch as sequential\n",
        "        #Try without using sequential, use each layer separately\n",
        "        #Can use without Flatten, average pool does the same thing\n",
        "        #Follow MultiMix code\n",
        "        self.classification = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), nn.Linear(channels[-1], 1), nn.Sigmoid()).to(device)\n",
        "\n",
        "        print(self.blocks)\n",
        "\n",
        "    def forward(self, X):\n",
        "        y = X.to(self.device)\n",
        "\n",
        "        skipConnections = []\n",
        "\n",
        "        for block in self.blocks:\n",
        "            y, skip = block(y)\n",
        "            skipConnections.append(skip)\n",
        "\n",
        "        return self.classification(y), skipConnections, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationNetwork(nn.Module):\n",
        "    def __init__(self, encoder, decoder, middleBlockInDim, middleBlockOutDim, endDim, dropout, device) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        self.middleBlock = convBlock(middleBlockInDim, middleBlockOutDim, True, 1, 0, 0.25, dropout)\n",
        "\n",
        "        self.endBlock = nn.Conv2d(endDim, 1, kernel_size=1, padding=0, stride=1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        _, skip, y = self.encoder(X)\n",
        "\n",
        "        y = self.middleBlock(y)\n",
        "        y = self.decoder(y, skip)\n",
        "\n",
        "        sigmoid = nn.Sigmoid()\n",
        "\n",
        "        return sigmoid(self.endBlock(y))"
      ],
      "metadata": {
        "id": "pdQpz5tB_4Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "s4TpJ7zaoLed"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98eUKvAr2KPz"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy(net, testIter, lossFunc, classification=True, device=None):\n",
        "    net.eval()\n",
        "    if not device:\n",
        "        device = next(iter(net.parameters())).device\n",
        "\n",
        "    #Accuracy, number of samples, loss\n",
        "    metric = d2l.Accumulator(2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, (X, y1, y2) in enumerate(testIter):\n",
        "            X = X.to(device)\n",
        "            y1 = y1.to(device)\n",
        "            y2 = y2.to(device)\n",
        "\n",
        "            yhat = net(X)[0]\n",
        "\n",
        "            if classification:\n",
        "                loss, accuracy = lossFunc(yhat, y2)\n",
        "            else:\n",
        "                loss, accuracy = lossFunc(yhat, y1)\n",
        "\n",
        "            metric.add(accuracy, loss)\n",
        "\n",
        "    return metric[0] / len(testIter), metric[1] / len(testIter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZgVGj5whycV"
      },
      "outputs": [],
      "source": [
        "def train(net: nn.Module, trainIter, testIter, numEpochs, startEpoch, learnRate, batchSize, device: torch.device, startDim, epochsToDouble, modelFileName, epochsToSave, \n",
        "          useWandB=False, cosineAnnealing=True, restartEpochs=-1, progressive=False, lossFunc = nn.BCEWithLogitsLoss(), classification=True):\n",
        "    print(f\"Training on {device}\")\n",
        "    \n",
        "    net.to(device)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learnRate)\n",
        "\n",
        "    #Setting restartEpochs to a negative will use no warm restarts, otherwise will use warm restarts \n",
        "    if cosineAnnealing:\n",
        "        if restartEpochs < 0:\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "        else:\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, restartEpochs, T_mult=1)\n",
        "\n",
        "    numBatches = len(trainIter)\n",
        "    bestValLoss = float('inf')\n",
        "\n",
        "    currDim = startDim\n",
        "    for epoch in range(startEpoch, numEpochs):\n",
        "        net.train()\n",
        "        \n",
        "        #Loss, accuracy\n",
        "        metric = d2l.Accumulator(2)\n",
        "\n",
        "        for i, (X, y1, y2) in enumerate(trainIter):\n",
        "            optimizer.zero_grad()\n",
        "            y1 = y1.to(device)\n",
        "            y2 = y2.to(device)\n",
        "\n",
        "            if progressive > 0:\n",
        "                #If using progressive learning, downsamples image to the current dimension\n",
        "                X = F.interpolate(X, size=int(currDim))\n",
        "\n",
        "            X = X.to(device)\n",
        "            \n",
        "            yhat = net(X)[0].to(device)\n",
        "\n",
        "            if classification:\n",
        "                l, accuracy = lossFunc(yhat, y2)\n",
        "            else:\n",
        "                l, accuracy = lossFunc(yhat, y1)\n",
        "\n",
        "            l.requires_grad_()\n",
        "\n",
        "            #print(f\"Loss: {l.item()} Predictions: {yhat.tolist()} Labels: {y.tolist()}\")\n",
        "\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if cosineAnnealing:\n",
        "                scheduler.step(epoch + i / numBatches)\n",
        "\n",
        "            metric.add(l, accuracy)\n",
        "\n",
        "        #Progressive learning\n",
        "        if (epoch + 1) % epochsToDouble == 0 and progressive == 1:\n",
        "            currDim *= 2\n",
        "        #Reverse progressive learning\n",
        "        elif (epoch + 1) % epochsToDouble == 0 and progressive == 2:\n",
        "            currDim /= 2\n",
        "\n",
        "        #Checkpoints model\n",
        "        if (epoch + 1) % epochsToSave == 0:\n",
        "            torch.save(net.state_dict(), modelFileName + \"Epoch\" + str(epoch))\n",
        "\n",
        "        validationAcc, validationLoss = evaluate_accuracy(net, testIter, lossFunc, classification=classification, device=device)\n",
        "\n",
        "        #Overwrites previous best model based on validation accuracy\n",
        "        if validationLoss < bestValLoss:\n",
        "            bestValLoss = validationLoss\n",
        "            torch.save(net.state_dict(), modelFileName + \"BestLoss\")\n",
        "\n",
        "        print(f\"Train Acc: {metric[1] / numBatches} Validation Acc: {validationAcc} Train Loss: {metric[0] / numBatches} Validation Loss: {validationLoss}\")\n",
        "\n",
        "        #Externally logs epoch info to WandB\n",
        "        if useWandB:\n",
        "            wandb.log({\"Train Acc\": metric[1] / numBatches,\n",
        "                    \"Validation Acc\": validationAcc,\n",
        "                    \"Train Loss\": metric[0] / numBatches,\n",
        "                    \"Validation Loss\": validationLoss\n",
        "                    })"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "Oa5VBt2woQSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbHyODbs1XNH"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters and training modifications\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "\n",
        "modelName = \"temp\"\n",
        "fileSaveName = \"/content/drive/MyDrive/\" + modelName\n",
        "\n",
        "#Use if starting from a checkpoint\n",
        "startEpoch = 0\n",
        "\n",
        "useWandB = False\n",
        "\n",
        "batchSize = 6\n",
        "learnRate = 0.001\n",
        "epochs = 100\n",
        "dropout = 0\n",
        "\n",
        "#Progressive training parameters\n",
        "startDim = 32\n",
        "epochsToDouble = 25\n",
        "\n",
        "#0: not progressive, 1: progressive, 2: reverse progressive\n",
        "progressive = 1\n",
        "\n",
        "#Checkpointing\n",
        "epochsToSave = 10\n",
        "\n",
        "#Learn rate scheduling parameters\n",
        "cosineAnnealing = True\n",
        "cosineRestartEpochs = 10\n",
        "\n",
        "#lossFunc = BalancedCELoss(weight0=1, weight1=1.5)\n",
        "#lossFunc = FocalLoss(weight0=0.2, weight1=0.8, gamma=2)\n",
        "#lossFunc = nn.BCEWithLogitsLoss()\n",
        "#lossFunc = WeightedDiceLoss(weight0=0.2, weight1=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0JenAeF1rFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c42739a-b9f7-4a87-8453-65c756d86f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded\n"
          ]
        }
      ],
      "source": [
        "#Load Datasets\n",
        "trainDataset = LITSBinaryDataset(\"drive/MyDrive/MachineLearningResearch/Datasets/FullTrainDataset.hdf5\")\n",
        "validationDataset = LITSBinaryDataset(\"drive/MyDrive/MachineLearningResearch/Datasets/ValidationDataset.hdf5\")\n",
        "testDataset = LITSBinaryDataset(\"drive/MyDrive/MachineLearningResearch/Datasets/TestDataset.hdf5\")\n",
        "\n",
        "trainIter = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)\n",
        "validationIter = DataLoader(validationDataset, batch_size=batchSize)\n",
        "testIter = DataLoader(testDataset, batch_size=batchSize)\n",
        "\n",
        "print(\"Dataset loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Joint Training**"
      ],
      "metadata": {
        "id": "blz2NJ-mo_vJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvELD2iw1fgj"
      },
      "outputs": [],
      "source": [
        "#Load model\n",
        "jointTrainingFileName = \"\"\n",
        "\n",
        "lossFunc = WeightedDiceLoss(weight0=0.2, weight1=0.8)\n",
        "\n",
        "encoder = EncoderNetwork([16, 32, 64, 128], 1, dropout, device).to(device)\n",
        "decoder = DecoderNetwork([256, 128, 64, 32, 16], 1, dropout, device).to(device)\n",
        "segmenter = SegmentationNetwork(encoder, decoder, 128, 256, 16, dropout, device)\n",
        "#print(summary(net, (1, 256, 256)))\n",
        "\n",
        "#Loads model from file if using a pretrained version\n",
        "if jointTrainingFileName != \"\":\n",
        "    segmenter.load_state_dict(torch.load(jointTrainingFileName))\n",
        "\n",
        "segmenter = segmenter.to(device)\n",
        "\n",
        "print(\"Intialized joint training model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YVz5cMohycW"
      },
      "outputs": [],
      "source": [
        "#Train Model\n",
        "gc.collect()\n",
        "\n",
        "if useWandB:\n",
        "    wandb.init(project=\"LiverSegmentation\",\n",
        "            name=modelName,\n",
        "            config={\n",
        "                \"BatchSize\":batchSize,\n",
        "                \"LearnRate\":learnRate,\n",
        "                \"Epochs\":epochs,\n",
        "                \"StartDimension\":startDim,\n",
        "                \"EpochsToDouble\":epochsToDouble\n",
        "            })\n",
        "\n",
        "train(segmenter, trainIter, validationIter, epochs, startEpoch, learnRate, batchSize, device, startDim, epochsToDouble, fileSaveName, epochsToSave, useWandB=useWandB, \n",
        "      cosineAnnealing=cosineAnnealing, restartEpochs=cosineRestartEpochs, progressive=progressive, lossFunc=lossFunc, classification=False)\n",
        "\n",
        "if useWandB:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pre-Training**"
      ],
      "metadata": {
        "id": "UMT_AzWypZdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lossFunc = FocalLoss(weight0=0.2, weight1=0.8, gamma=2)\n",
        "\n",
        "block1 = EncoderBlock(1, 16, 1, 0.2)\n",
        "block2 = EncoderBlock(16, 32, 1, 0.2)\n",
        "block3 = EncoderBlock(32, 64, 1, 0.2)\n",
        "block4 = EncoderBlock(64, 128, 1, 0.2)\n",
        "block5 = EncoderBlock(128, 256, 1, 0.2)\n",
        "\n",
        "encoder = nn.Sequential(block1, block2, block3, block4, block5, nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), nn.Linear(256, 1), nn.Sigmoid())\n",
        "\n",
        "print(encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyW0yEbEsQ-3",
        "outputId": "98e1b358-607a-4978-f4e0-53f70e181e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): EncoderBlock(\n",
            "    (conv): convBlock(\n",
            "      (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (1): EncoderBlock(\n",
            "    (conv): convBlock(\n",
            "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (2): EncoderBlock(\n",
            "    (conv): convBlock(\n",
            "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (3): EncoderBlock(\n",
            "    (conv): convBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (4): EncoderBlock(\n",
            "    (conv): convBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout): Dropout(p=0.2, inplace=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (5): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  (7): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (8): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lossFunc = FocalLoss(weight0=0.2, weight1=0.8, gamma=2)\n",
        "\n",
        "encoderFileName = \"\"\n",
        "\n",
        "encoder = EncoderNetwork([16, 32, 64, 128, 256], 1, dropout, device).to(device)\n",
        "\n",
        "if encoderFileName != \"\":\n",
        "    encoder.load_state_dict(torch.load(encoderFileName))\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "\n",
        "print(encoder)"
      ],
      "metadata": {
        "id": "JsMuX0NYpYxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4233cf19-e5f3-451d-bc05-e5f179277ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EncoderBlock(\n",
            "  (conv): convBlock(\n",
            "    (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (dropout): Dropout(p=0, inplace=False)\n",
            "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "), EncoderBlock(\n",
            "  (conv): convBlock(\n",
            "    (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (dropout): Dropout(p=0, inplace=False)\n",
            "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "), EncoderBlock(\n",
            "  (conv): convBlock(\n",
            "    (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (dropout): Dropout(p=0, inplace=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "), EncoderBlock(\n",
            "  (conv): convBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (dropout): Dropout(p=0, inplace=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "), EncoderBlock(\n",
            "  (conv): convBlock(\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (dropout): Dropout(p=0, inplace=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")]\n",
            "EncoderNetwork(\n",
            "  (classification): Sequential(\n",
            "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (1): Flatten(start_dim=1, end_dim=-1)\n",
            "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (3): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Model\n",
        "gc.collect()\n",
        "\n",
        "if useWandB:\n",
        "    wandb.init(project=\"LiverClassifier2\",\n",
        "            name=\"Progressive\" + str(dropout),\n",
        "            config={\n",
        "                \"BatchSize\":batchSize,\n",
        "                \"LearnRate\":learnRate,\n",
        "                \"Epochs\":epochs,\n",
        "                \"StartDimension\":startDim,\n",
        "                \"EpochsToDouble\":epochsToDouble,\n",
        "                \"Dropout\":dropout,\n",
        "            })\n",
        "\n",
        "train(encoder, trainIter, validationIter, epochs, startEpoch, learnRate, batchSize, device, startDim, epochsToDouble, fileSaveName, epochsToSave, useWandB=useWandB, \n",
        "      cosineAnnealing=cosineAnnealing, restartEpochs=cosineRestartEpochs, progressive=progressive, lossFunc=lossFunc, classification=True)\n",
        "\n",
        "if useWandB:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "NR1JX1AeqQAO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "92e65dc3-4d35-4f58-8d81-8fd91cd8a5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on cuda\n",
            "Train Acc: 0.5743405275779381 Validation Acc: 0.25 Train Loss: 0.05551540767170971 Validation Loss: 0.06152976602315903\n",
            "Train Acc: 0.7134292565947244 Validation Acc: 0.25 Train Loss: 0.03979905240787126 Validation Loss: 0.05894099064171314\n",
            "Train Acc: 0.7517985611510797 Validation Acc: 0.25 Train Loss: 0.03745648845789029 Validation Loss: 0.06275923043489456\n",
            "Train Acc: 0.7601918465227823 Validation Acc: 0.25 Train Loss: 0.034867453258216594 Validation Loss: 0.06636136949062348\n",
            "Train Acc: 0.7871702637889691 Validation Acc: 0.25 Train Loss: 0.03307217086678733 Validation Loss: 0.05905896313488483\n",
            "Train Acc: 0.8093525179856119 Validation Acc: 0.6683333333333333 Train Loss: 0.030552252525758508 Validation Loss: 0.059511965662240984\n",
            "Train Acc: 0.8045563549160678 Validation Acc: 0.31 Train Loss: 0.029396616269184425 Validation Loss: 0.058966809734702114\n",
            "Train Acc: 0.8123501199040768 Validation Acc: 0.25 Train Loss: 0.028873087458895427 Validation Loss: 0.058768841847777364\n",
            "Train Acc: 0.824340527577938 Validation Acc: 0.25 Train Loss: 0.02886173819533462 Validation Loss: 0.05886531919240952\n",
            "Train Acc: 0.8183453237410079 Validation Acc: 0.25 Train Loss: 0.028370652398054332 Validation Loss: 0.05884758725762367\n",
            "Train Acc: 0.809952038369305 Validation Acc: 0.25 Train Loss: 0.030136036968006076 Validation Loss: 0.05884306199848652\n",
            "Train Acc: 0.8243405275779382 Validation Acc: 0.25 Train Loss: 0.02888203543491943 Validation Loss: 0.06022113963961601\n",
            "Train Acc: 0.8189448441247013 Validation Acc: 0.6483333333333334 Train Loss: 0.029267252900255777 Validation Loss: 0.05939336061477661\n",
            "Train Acc: 0.8291366906474829 Validation Acc: 0.35666666666666663 Train Loss: 0.027834812269659993 Validation Loss: 0.05886836625635624\n",
            "Train Acc: 0.8495203836930463 Validation Acc: 0.25 Train Loss: 0.02595305040130298 Validation Loss: 0.06341278158128262\n",
            "Train Acc: 0.8339328537170271 Validation Acc: 0.7533333333333334 Train Loss: 0.025187780596516867 Validation Loss: 0.06228737585246563\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-7abed63955a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             })\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m train(encoder, trainIter, validationIter, epochs, startEpoch, learnRate, batchSize, device, startDim, epochsToDouble, fileSaveName, epochsToSave, useWandB=useWandB, \n\u001b[0m\u001b[1;32m     17\u001b[0m       cosineAnnealing=cosineAnnealing, restartEpochs=cosineRestartEpochs, progressive=progressive, lossFunc=lossFunc, classification=True)\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-ff8ff673b4e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, trainIter, testIter, numEpochs, startEpoch, learnRate, batchSize, device, startDim, epochsToDouble, modelFileName, epochsToSave, useWandB, cosineAnnealing, restartEpochs, progressive, lossFunc, classification)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m#print(f\"Loss: {l.item()} Predictions: {yhat.tolist()} Labels: {y.tolist()}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lossFunc = WeightedDiceLoss(weight0=0.2, weight1=0.8)\n",
        "\n",
        "encoder.load_state_dict(torch.load(fileSaveName + \"BestLoss\"))\n",
        "decoder = DecoderNetwork([256, 128, 64, 32, 16], 1, dropout, device).to(device)\n",
        "segmenter = SegmentationNetwork(encoder, decoder, 128, 256, 16, dropout, device)\n",
        "\n",
        "segmenter = segmenter.to(device)"
      ],
      "metadata": {
        "id": "PLEB6MZfqk4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Model\n",
        "gc.collect()\n",
        "\n",
        "if useWandB:\n",
        "    wandb.init(project=\"LiverSegmentation\",\n",
        "            name=modelName + \"Segmenter\",\n",
        "            config={\n",
        "                \"BatchSize\":batchSize,\n",
        "                \"LearnRate\":learnRate,\n",
        "                \"Epochs\":epochs,\n",
        "                \"StartDimension\":startDim,\n",
        "                \"EpochsToDouble\":epochsToDouble\n",
        "            })\n",
        "\n",
        "train(segmenter, trainIter, validationIter, epochs, startEpoch, learnRate, batchSize, device, startDim, epochsToDouble, fileSaveName, epochsToSave, useWandB=useWandB, \n",
        "      cosineAnnealing=cosineAnnealing, restartEpochs=cosineRestartEpochs, progressive=progressive, lossFunc=lossFunc, classification=False)\n",
        "\n",
        "if useWandB:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "ti-Rck8Xq3sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation/Ending**"
      ],
      "metadata": {
        "id": "iipij4bjp9hs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1tLSjzH2F-Z"
      },
      "outputs": [],
      "source": [
        "classification = True\n",
        "\n",
        "if classification:\n",
        "    net = encoder\n",
        "else:\n",
        "    net = segmenter\n",
        "\n",
        "#Evaluate Model\n",
        "print(f\"Model: {modelName}\")\n",
        "\n",
        "trainAcc, trainLoss = evaluate_accuracy(net, trainIter, lossFunc, classification=classification, device=device)\n",
        "print(f\"Train Accuracy: {trainAcc} Train Loss: {trainLoss}\")\n",
        "\n",
        "validationAcc, validationLoss = evaluate_accuracy(net, validationIter, lossFunc, classification=classification, device=device)\n",
        "print(f\"Validation Accuracy: {validationAcc} Validation Loss: {validationLoss}\")\n",
        "\n",
        "testAcc, testLoss = evaluate_accuracy(net, testIter, lossFunc, classification=classification, device=device)\n",
        "print(f\"Test Accuracy: {testAcc} Test Loss: {testLoss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LDAG1Pd2KXN"
      },
      "outputs": [],
      "source": [
        "#Close datasets\n",
        "trainDataset.closeFile()\n",
        "validationDataset.closeFile()\n",
        "testDataset.closeFile()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IGcZ_Vm5n5Yv",
        "6g1MsLCCoDhb",
        "s4TpJ7zaoLed",
        "iipij4bjp9hs"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ea23a5d6ea5dd6b47a6bacc48f8acbc6e91dc182fa6b25270d70228f0691131c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
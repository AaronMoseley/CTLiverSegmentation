{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbV9155tKF7w",
        "outputId": "2f70abef-f597-48fe-b05a-60e8f6a0399d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamoseley018\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Only neccessary if logging performance data on wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UySZvCsHhycT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "import h5py\n",
        "from UNet import UNet, Encoder, ContrastiveEncoder, ResidualBlock, double_conv, ResUNet\n",
        "from LITSDataset import LITSBinaryDataset, LITSContDatasetPolyCL, LITSContDatasetSimCLR, LITSMultiClassDataset\n",
        "import LossFunctions\n",
        "import TrainingEval\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tbHyODbs1XNH"
      },
      "outputs": [],
      "source": [
        "#Second line only used if experiencing serious problems with using a gpu or if a gpu is unavailable (not recommended)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "\n",
        "modelName = \"ResUNetTest1\"\n",
        "modelFile = \"UsedModels/\" + modelName\n",
        "\n",
        "#Specify the file location of the config file, this contains all hyperparameters for training the model\n",
        "#Everything below loads the data from the config file\n",
        "configFile = \"../testConfig.txt\"\n",
        "\n",
        "startEpoch = 0\n",
        "useWandB = 0\n",
        "batchSize = 0\n",
        "learnRate = 0\n",
        "epochs = 0\n",
        "startDim = 0\n",
        "epochsToDouble = 0\n",
        "progressive = 0\n",
        "epochsToSave = 0\n",
        "cosineAnnealing = 0\n",
        "cosineRestartEpochs = 0\n",
        "\n",
        "varDict = {\n",
        "    \"startEpoch\":startEpoch,\n",
        "    \"useWandB\":useWandB,\n",
        "    \"batchSize\":batchSize,\n",
        "    \"learnRate\":learnRate,\n",
        "    \"epochs\":epochs,\n",
        "    \"startDim\":startDim,\n",
        "    \"epochsToDouble\":epochsToDouble,\n",
        "    \"progressive\":progressive,\n",
        "    \"epochsToSave\":epochsToSave,\n",
        "    \"cosineAnnealing\":cosineAnnealing,\n",
        "    \"cosineRestartEpochs\":cosineRestartEpochs,\n",
        "}\n",
        "\n",
        "TrainingEval.ParseConfig(configFile, varDict)\n",
        "\n",
        "for key in varDict:\n",
        "    if varDict[key].is_integer():\n",
        "        locals()[key] = int(varDict[key])\n",
        "    else:\n",
        "        locals()[key] = varDict[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Loads binary datasets\n",
        "trainDataset = LITSBinaryDataset(\"../Datasets/StandardDatasets/FullTrainDataset.hdf5\")\n",
        "validationDataset = LITSBinaryDataset(\"../Datasets/StandardDatasets/ValidationDataset.hdf5\")\n",
        "testDataset = LITSBinaryDataset(\"../Datasets/StandardDatasets/TestDataset.hdf5\")\n",
        "\n",
        "trainIter = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)\n",
        "validationIter = DataLoader(validationDataset, batch_size=batchSize)\n",
        "testIter = DataLoader(testDataset, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0JenAeF1rFu",
        "outputId": "cfa27150-7236-40f2-ac04-f7babcee52a2"
      },
      "outputs": [],
      "source": [
        "#Loads binary datasets\n",
        "trainDataset = LITSBinaryDataset(\"../Datasets/FullLiTS/FullLiTSTrainingDataset.hdf5\")\n",
        "validationDataset = LITSBinaryDataset(\"../Datasets/FullLiTS/FullLiTSValidationDataset.hdf5\")\n",
        "testDataset = LITSBinaryDataset(\"../Datasets/FullLiTS/FullLiTSTestingDataset.hdf5\")\n",
        "\n",
        "trainIter = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)\n",
        "validationIter = DataLoader(validationDataset, batch_size=batchSize)\n",
        "testIter = DataLoader(testDataset, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Loads multiclass datasets\n",
        "trainDataset = LITSMultiClassDataset(\"Datasets/MultiClass/MultiClassTrainingDataset.hdf5\")\n",
        "validationDataset = LITSMultiClassDataset(\"Datasets/MultiClass/MultiClassValidationDataset.hdf5\")\n",
        "testDataset = LITSMultiClassDataset(\"Datasets/MultiClass/MultiClassTestingDataset.hdf5\")\n",
        "\n",
        "trainIter = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)\n",
        "validationIter = DataLoader(validationDataset, batch_size=batchSize)\n",
        "testIter = DataLoader(testDataset, batch_size=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "blz2NJ-mo_vJ"
      },
      "source": [
        "# **Standard Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    modelFile = \"../UsedModels/ResUNetBaseline\" + str(i)\n",
        "    \n",
        "    #Specifies the loss functions and weights to use during training process\n",
        "    #Example: \n",
        "    #   lossFuncs = [[segmentationLossFunc1, segmentationLossFunc2], [classificationLossFunc1, classificationLossFunc2]]\n",
        "    #   weights = [[0.25, 0.5], [0.1, 0.9]]\n",
        "    #Allows for easily changing the loss function and enables joint training on segmentation and classification\n",
        "    #Loss functions given weights of 0 are printed every epoch but are not included in the loss calculation\n",
        "    lossFuncs = [[LossFunctions.dice_loss, LossFunctions.binary_pixel_ce, LossFunctions.dice_score], []]\n",
        "    weights = [[0.5, 0.5, 0], []]\n",
        "\n",
        "    segmenter = ResUNet(num_classes=1).to(device)\n",
        "\n",
        "    #Loads model from file if using a pretrained version\n",
        "    initModel = \"\"\n",
        "    if initModel != \"\":\n",
        "        segmenter.load_state_dict(torch.load(initModel))\n",
        "\n",
        "    #If the config file specifies using WandB, begins the run\n",
        "    if useWandB:\n",
        "        wandb.init(project=\"EMBCBaseline\",\n",
        "                name=modelName,\n",
        "                config={\n",
        "                    \"BatchSize\":batchSize,\n",
        "                    \"LearnRate\":learnRate,\n",
        "                    \"Epochs\":epochs,\n",
        "                    \"StartDimension\":startDim,\n",
        "                    \"EpochsToDouble\":epochsToDouble\n",
        "                })\n",
        "\n",
        "    TrainingEval.train(segmenter, lossFuncs, weights, trainIter, validationIter, epochs, startEpoch, learnRate, device, startDim, epochsToDouble, modelFile, epochsToSave, useWandB=useWandB, \n",
        "        cosineAnnealing=cosineAnnealing, restartEpochs=cosineRestartEpochs, progressive=progressive)\n",
        "\n",
        "    if useWandB:\n",
        "        wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UMT_AzWypZdL"
      },
      "source": [
        "# **Classification Pre-Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR1JX1AeqQAO"
      },
      "outputs": [],
      "source": [
        "#Specifies the loss functions and weights to use during training process\n",
        "#Example: \n",
        "#   lossFuncs = [[segmentationLossFunc1, segmentationLossFunc2], [classificationLossFunc1, classificationLossFunc2]]\n",
        "#   weights = [[0.25, 0.5], [0.1, 0.9]]\n",
        "#Allows for easily changing the loss function and enables joint training on segmentation and classification\n",
        "#Loss functions given weights of 0 are printed every epoch but are not included in the loss calculation\n",
        "\n",
        "focal = LossFunctions.FocalLoss(weight0=0.1, weight1=0.9, gamma=2)\n",
        "lossFuncs = [[], [focal, LossFunctions.accuracy, LossFunctions.f1]]\n",
        "weights = [[], [1, 0, 0]]\n",
        "\n",
        "#Saves encoder model to separate file than the modelFile specified above\n",
        "encoderFile = \"UsedModels/Encoder1\"\n",
        "\n",
        "#Creates UNet encoder with classification branch\n",
        "encoder = Encoder(1).to(device)\n",
        "\n",
        "#Loads encoder model if one already exists\n",
        "initEncoder = \"\"\n",
        "if initEncoder != \"\":\n",
        "    encoder.load_state_dict(torch.load(initEncoder))\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "#Starts WandB run if that is being used\n",
        "if useWandB:\n",
        "    wandb.init(project=\"PreTrainedEncoder\",\n",
        "            name=\"UNetEncoder\",\n",
        "            config={\n",
        "                \"BatchSize\":batchSize,\n",
        "                \"LearnRate\":learnRate,\n",
        "                \"Epochs\":epochs,\n",
        "                \"StartDimension\":startDim,\n",
        "                \"EpochsToDouble\":epochsToDouble,\n",
        "            })\n",
        "\n",
        "print(TrainingEval.train(encoder, lossFuncs, weights, trainIter, validationIter, epochs, startEpoch, learnRate, device, startDim, epochsToDouble, encoderFile, epochsToSave, useWandB=useWandB, \n",
        "      cosineAnnealing=cosineAnnealing, restartEpochs=cosineRestartEpochs, progressive=progressive, encoder=True))\n",
        "\n",
        "if useWandB:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti-Rck8Xq3sj"
      },
      "outputs": [],
      "source": [
        "#Creates and loads encoder file from initEncoder, uses that when creating the full UNet model\n",
        "encoder = Encoder(1).to(device)\n",
        "\n",
        "initEncoder = \"\"\n",
        "if initEncoder != \"\":\n",
        "    encoder.load_state_dict(torch.load(initEncoder))\n",
        "\n",
        "segmenter = UNet(encoder=encoder)\n",
        "\n",
        "#After loading encoder, model is trained in the same way as standard models\n",
        "lossFuncs = [[LossFunctions.dice_loss, LossFunctions.dice_score], []]\n",
        "weights = [[1, 0], []]\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "if useWandB:\n",
        "    wandb.init(project=\"LiverSegmentationPreTraining\",\n",
        "            name=\"NoWeights\",\n",
        "            config={\n",
        "                \"BatchSize\":batchSize,\n",
        "                \"LearnRate\":learnRate,\n",
        "                \"Epochs\":epochs,\n",
        "                \"StartDimension\":startDim,\n",
        "                \"EpochsToDouble\":epochsToDouble\n",
        "            })\n",
        "\n",
        "TrainingEval.train(segmenter, lossFuncs, weights, trainIter, validationIter, epochs, startEpoch, learnRate, device, startDim, epochsToDouble, modelFile, epochsToSave, useWandB=useWandB, \n",
        "      cosineAnnealing=cosineAnnealing, restartEpochs=cosineRestartEpochs, progressive=progressive)\n",
        "\n",
        "if useWandB:\n",
        "    wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JH2TvySW9ux8"
      },
      "source": [
        "# **Joint Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "st2leTac4VsP",
        "outputId": "0ee0bbc1-6d44-4e32-8cfa-d737f988c191"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    modelFile = \"../UsedModels/JointTrainedResUNet\" + str(i)\n",
        "\n",
        "    #Specifies the loss functions and their weights to use during training process (both segmentation and classification)\n",
        "    #Example: \n",
        "    #   lossFuncs = [[segmentationLossFunc1, segmentationLossFunc2], [classificationLossFunc1, classificationLossFunc2]]\n",
        "    #   weights = [[0.25, 0.5], [0.1, 0.9]]\n",
        "    #Allows for easily changing the loss function and enables joint training on segmentation and classification\n",
        "    #Loss functions given weights of 0 are printed every epoch but are not included in the loss calculation\n",
        "    classLossFunc = LossFunctions.FocalLoss(weight0=0.2, weight1=0.8, gamma=2)\n",
        "    lossFuncs = [[LossFunctions.dice_score, LossFunctions.dice_loss], [LossFunctions.accuracy, classLossFunc]]\n",
        "    weights = [[0, 0.6], [0, 0.4]]\n",
        "\n",
        "    segmenter = ResUNet(num_classes=1).to(device)\n",
        "\n",
        "    #Loads model from file if using a pretrained version\n",
        "    initModel = \"\"\n",
        "    if initModel != \"\":\n",
        "        segmenter.load_state_dict(torch.load(initModel))\n",
        "\n",
        "    #Uses WandB to log run data if specified by the config file\n",
        "    if useWandB:\n",
        "        wandb.init(project=\"EMBCJointTraining\",\n",
        "                name=\"JointResUNet\" + str(i),\n",
        "                config={\n",
        "                    \"BatchSize\":batchSize,\n",
        "                    \"LearnRate\":learnRate,\n",
        "                    \"Epochs\":epochs,\n",
        "                    \"StartDimension\":startDim,\n",
        "                    \"EpochsToDouble\":epochsToDouble\n",
        "                })\n",
        "\n",
        "    TrainingEval.train(segmenter, lossFuncs, weights, trainIter, validationIter, epochs, startEpoch, learnRate, device, startDim, epochsToDouble, modelFile, epochsToSave, useWandB=useWandB, \n",
        "        cosineAnnealing=cosineAnnealing, restartEpochs=cosineRestartEpochs, progressive=progressive)\n",
        "\n",
        "    if useWandB:\n",
        "        wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Contrastive Pre-Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Loads contrastive training dataset\n",
        "contTrainDataset = LITSContDatasetPolyCL(\"Datasets/ContrastiveDatasets/SimCLRTrainingDataset.hdf5\")\n",
        "contTrainIter = DataLoader(contTrainDataset, batch_size=batchSize, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Creates an encoder with a projection head for contrastive learning\n",
        "encoder = ContrastiveEncoder()\n",
        "\n",
        "if useWandB:\n",
        "    wandb.init(project=\"LITSEncoderContrastive\",\n",
        "            name=\"Weights:\",\n",
        "            config={\n",
        "                \"BatchSize\":batchSize,\n",
        "                \"LearnRate\":learnRate,\n",
        "                \"Epochs\":epochs,\n",
        "                \"StartDimension\":startDim,\n",
        "                \"EpochsToDouble\":epochsToDouble\n",
        "            })\n",
        "    \n",
        "lossFunc = LossFunctions.ContrastiveLossSimCLR(temp=(1 / batchSize), device=device)\n",
        "\n",
        "#Uses specific SimCLR training function because of the differences between it and the PolyCL pre-training process\n",
        "TrainingEval.simCLRTrain(encoder, lossFunc, contTrainIter, epochs, startEpoch, learnRate, device, modelFile, epochsToSave, useWandB=useWandB, cosineAnnealing=cosineAnnealing, restartEpochs=cosineRestartEpochs)\n",
        "\n",
        "if useWandB:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Creates an encoder with a projection head for contrastive learning\n",
        "encoder = ContrastiveEncoder()\n",
        "\n",
        "if useWandB:\n",
        "    wandb.init(project=\"LITSEncoderContrastive\",\n",
        "            name=\"Weights:\",\n",
        "            config={\n",
        "                \"BatchSize\":batchSize,\n",
        "                \"LearnRate\":learnRate,\n",
        "                \"Epochs\":epochs,\n",
        "                \"StartDimension\":startDim,\n",
        "                \"EpochsToDouble\":epochsToDouble\n",
        "            })\n",
        "    \n",
        "lossFunc = LossFunctions.ContrastiveLossCosine(temp=(1 / batchSize))\n",
        "\n",
        "#Uses PolyCL contrastive training function\n",
        "TrainingEval.contrastiveTrain(encoder, lossFunc, contTrainIter, epochs, startEpoch, learnRate, device, modelFile, epochsToSave, useWandB=useWandB, cosineAnnealing=cosineAnnealing, restartEpochs=cosineRestartEpochs)\n",
        "\n",
        "if useWandB:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Creates and loads encoder file from initEncoder, uses that when creating the full UNet model\n",
        "encoder = ContrastiveEncoder().to(device)\n",
        "\n",
        "initEncoder = \"\"\n",
        "if initEncoder != \"\":\n",
        "    encoder.load_state_dict(torch.load(initEncoder))\n",
        "\n",
        "segmenter = UNet(encoder=encoder)\n",
        "\n",
        "#After loading encoder, model is trained in the same way as standard models\n",
        "lossFuncs = [[LossFunctions.dice_loss, LossFunctions.dice_score], []]\n",
        "weights = [[1, 0], []]\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "if useWandB:\n",
        "    wandb.init(project=\"LiverSegmentationPreTraining\",\n",
        "            name=\"NoWeights\",\n",
        "            config={\n",
        "                \"BatchSize\":batchSize,\n",
        "                \"LearnRate\":learnRate,\n",
        "                \"Epochs\":epochs,\n",
        "                \"StartDimension\":startDim,\n",
        "                \"EpochsToDouble\":epochsToDouble\n",
        "            })\n",
        "\n",
        "TrainingEval.train(segmenter, lossFuncs, weights, trainIter, validationIter, epochs, startEpoch, learnRate, device, startDim, epochsToDouble, modelFile, epochsToSave, useWandB=useWandB, \n",
        "      cosineAnnealing=cosineAnnealing, restartEpochs=cosineRestartEpochs, progressive=progressive)\n",
        "\n",
        "if useWandB:\n",
        "    wandb.finish()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iipij4bjp9hs"
      },
      "source": [
        "# **Evaluation/Ending**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1tLSjzH2F-Z",
        "outputId": "2f548a4b-f201-45cc-e02e-1c90911370e8"
      },
      "outputs": [],
      "source": [
        "#Will evaluate all models in each directory listed here\n",
        "dirs = [\"FullLiTSTesting/\"]\n",
        "\n",
        "#Loops through each directory and each model\n",
        "for dir in dirs:\n",
        "    for modelName in os.listdir(dir):\n",
        "        modelFile = dir + modelName\n",
        "\n",
        "        #Can evaluate on multiple loss functions, listed here\n",
        "        #Specified in the same way as loss functions for training\n",
        "        #Segmentation loss functions are listed first, classification loss functions are listed after\n",
        "        lossFuncs = [[LossFunctions.dice_score, LossFunctions.hausdorff], []]\n",
        "\n",
        "        #If the model is an encoder and we are only evaluating on classification, an encoder is loaded, otherwise the full UNet is loaded\n",
        "        classification = False\n",
        "        if classification:\n",
        "            net = Encoder()\n",
        "            net.load_state_dict(torch.load(modelFile), strict=False)\n",
        "        else:\n",
        "            net = UNet(device, n_class=1, multiTask=False).to(device)\n",
        "            net.load_state_dict(torch.load(modelFile), strict=False)\n",
        "\n",
        "        print(f\"Model: {modelName}\")\n",
        "\n",
        "        #Evaluates each model on all losses, prints out the function names and the evaluated value\n",
        "        losses = TrainingEval.evaluate(net, testIter, lossFuncs, device=device, encoder=encoder)\n",
        "        logStr = \"\"\n",
        "        for i, arr in enumerate(losses):\n",
        "            for j, val in enumerate(arr):\n",
        "                logStr += (lossFuncs[i][j].__name__ if str(type(lossFuncs[i][j])) == \"<class 'function'>\" else type(lossFuncs[i][j]).__name__) + \": \" + str(val) + \" \"\n",
        "\n",
        "        print(logStr)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IGcZ_Vm5n5Yv",
        "6g1MsLCCoDhb",
        "6adquwMWoH0B",
        "D2RziTUxY8jV",
        "s4TpJ7zaoLed",
        "blz2NJ-mo_vJ",
        "UMT_AzWypZdL"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "ea23a5d6ea5dd6b47a6bacc48f8acbc6e91dc182fa6b25270d70228f0691131c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
